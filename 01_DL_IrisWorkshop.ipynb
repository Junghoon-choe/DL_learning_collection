{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Iris Data Loader and DataFrame Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'C:\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris() # sklearn.datasets에 저장된 데이터 불러옴\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write Code !!\n",
    "pd.DataFrame(data = iris['data'], columns=iris['feature_names']) # X(feature의 값들을 불러옴)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X,y data Generator...Feature and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Write Code !!\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Test 데이타를 8:2로 비율로 섞고, random_state=42로 지정\n",
    "    X_train, X_test, y_train, y_test 로 각각 할당된 값들을 torch 타입으로 변환 \n",
    "    torch.FloatTensor(), torch.LongTensor 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [6.  3.  4.8 1.8]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.  2.2 4.  1. ]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.7 3.  5.  1.7]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [6.  2.2 5.  1.5]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [7.1 3.  5.9 2.1]]\n",
      "[[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.8 3.1 1.6 0.2]]\n",
      "[0 0 1 0 0 2 1 0 0 0 2 1 1 0 0 1 2 2 1 2 1 2 1 0 2 1 0 0 0 1 2 0 0 0 1 0 1\n",
      " 2 0 1 2 0 2 2 1 1 2 1 0 1 2 0 0 1 1 0 2 0 0 1 1 2 1 2 2 1 0 0 2 2 0 0 0 1\n",
      " 2 0 2 2 0 1 1 2 1 2 0 2 1 2 1 1 1 0 1 1 0 1 2 2 0 1 2 2 0 2 0 1 2 2 1 2 1\n",
      " 1 2 2 0 1 2 0 1 2]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Code !!\n",
    "# 각 변수의 타입을 실수형텐서와 롱타입의텐서로 변경해준다.\n",
    "# 이유는 좀더 정말한 작업을 하기 위해서?\n",
    "X_train = torch.FloatTensor(X_train) \n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼파라미터 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4 # 2x2사이즈? 들어가는 인풋의 사이즈를 4로 지정한다.  \n",
    "hidden_size = 56 # 중간출력, 히든레이어의 출력값이 hidden_size가 된다.\n",
    "num_classes = 3 # 나올 클래스의 갯수를 선언\n",
    "num_epochs = 100 # 데이트를 얼마나 반복할지 선언\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork  Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, num_classes): \n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.fc1=nn.Linear(input_size, hidden_size) # (인풋, 아웃풋)\n",
    "        # ReLU넣기 중간 출력값 나오기전에\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2=nn.Linear(hidden_size,num_classes) # (인풋, 아웃풋)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       # Write Code !!\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNetwork  Model Excution , loss, optimizer, backward ..\n",
    "    Forward Propagation and Baward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "loss_function = nn.CrossEntropyLoss() # 이안에 softmax(), -log(x) ,R(W) 함수들이 내장되어 있다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # model.parameters()은 w,b를 해킹하고 있다. 그래서 back에서 가중치를 줄때 책임을 바로바로 묻게 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss is 1.1423\n",
      "epoch 10, loss is 0.3540\n",
      "epoch 20, loss is 0.1226\n",
      "epoch 30, loss is 0.0664\n",
      "epoch 40, loss is 0.0601\n",
      "epoch 50, loss is 0.0568\n",
      "epoch 60, loss is 0.0560\n",
      "epoch 70, loss is 0.0554\n",
      "epoch 80, loss is 0.0547\n",
      "epoch 90, loss is 0.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    }
   ],
   "source": [
    "# Write Code !!\n",
    "loss_list = []\n",
    "epochs_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    outputs = model(X_train) # 모델에 트레이닝할 값을 넣고 아웃풋을 출력해낸다.\n",
    "    loss = loss_function(outputs, y_train) # 출력된 아웃풋과 답을 비교하는 loss_function을 사용해서 loss값을 알아낸다.\n",
    "    \n",
    "    optimizer.zero_grad() # optimizer을 초기화 시킨다. optimizer를 사용할 준비를 함.\n",
    "    loss.backward() # loss값을 수정\n",
    "    optimizer.step() # step 함수로 수정된 w,b을 이용해 다시 학습을 시켜준다.\n",
    "    loss_list.append(loss.item())\n",
    "    epochs_list.append(epoch)\n",
    "    if epoch%10==0:# 학습은 100번을 반복합니다 학습이 진행됨에 따라서 Loss가 감소하는 것을 볼수 있도록 출력합니다.\n",
    "        print(f'epoch {epoch}, loss is {loss.item():,.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch(학습)에 따른 Loss감소를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write Code !!\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 통해서 Loss를 감소시켰다면 이제는Test를 해봅니다.\n",
    "    테스트 할때는 학습의 의미가 없기때문에 Gradient Descent를 사용하지 않도록 합니다.\n",
    "    그 결과로 컴퓨터 Performance를 높이는 결과를 가져옵니다.\n",
    "    이때 우리가 테스트하는 데이타는 이미지가 아니고 단순 숫자 값으로 입력된다는 점을 잘 고려해야합니다.\n",
    "    출력된 값 중에서 가장 높은 값의 인덱스가 바로 target의 라벨이 됩니다.\n",
    "    \n",
    "    예측한 값과 정답을 일일이 비교해서 출력하고\n",
    "    총 30개의 Test 데이타 중에서 정확하게 맞춘 갯수를 최종적으로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값, 정답 출력\n",
      "0) 1,1\n",
      "1) 0,0\n",
      "2) 2,2\n",
      "3) 1,1\n",
      "4) 1,1\n",
      "5) 0,0\n",
      "6) 1,1\n",
      "7) 2,2\n",
      "8) 1,1\n",
      "9) 1,1\n",
      "10) 2,2\n",
      "11) 0,0\n",
      "12) 0,0\n",
      "13) 0,0\n",
      "14) 0,0\n",
      "15) 1,1\n",
      "16) 2,2\n",
      "17) 1,1\n",
      "18) 1,1\n",
      "19) 2,2\n",
      "20) 0,0\n",
      "21) 2,2\n",
      "22) 0,0\n",
      "23) 2,2\n",
      "24) 2,2\n",
      "25) 2,2\n",
      "26) 2,2\n",
      "27) 2,2\n",
      "28) 0,0\n",
      "29) 0,0\n",
      "30개의 Test 데이타 중에서 정답을 맞춘 갯수는 30 개 입니다!!\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    # Write Code!!   \n",
    "    \n",
    "    out = model(X_test)\n",
    "    \n",
    "    _, pred = torch.max(out,1)\n",
    "    score = sum(pred == y_test).item() # item()은 값이 하나 일때만 가능하다.\n",
    "    print('예측값, 정답 출력')\n",
    "    for i in range(30):\n",
    "        print(f'{i}) {y_test[i]},{pred[i]}')\n",
    "        \n",
    "    print(f'30개의 Test 데이타 중에서 정답을 맞춘 갯수는 {score} 개 입니다!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
